{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import r2_score,mean_absolute_error\n",
    "from sklearn import linear_model\n",
    "import time\n",
    "import scipy.stats as sps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Files needed to extract from BigQuery weekly: (1) csv files of order records upto the cutoff date for top10 items (2) daily active users csv file upto the cutoff date (3) sellout feature upto week n-2 (endInventory table)\n",
    "\n",
    "* Cutoff date for week n: Sunday of week n-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unifDate(d):\n",
    "    '''\n",
    "    This function takes a date string in month/date/Year pattern,\n",
    "    and returns a date string in Year-month-day pattern.\n",
    "    '''\n",
    "    return datetime.datetime.strptime(d, \"%m/%d/%Y\").strftime(\"%Y-%m-%d\")\n",
    "\n",
    "def datemod(data):\n",
    "    '''\n",
    "    This function takes the whole dataset and converts all order dates into \n",
    "    adjusted business days stored in a list. \n",
    "    '''\n",
    "    Datemod = []\n",
    "    for i in range(len(data)):\n",
    "        hour, minute, sec = data.orderTime.iloc[i].split(':')\n",
    "        hour = int(hour)\n",
    "        if i == 0 or hour > 2:\n",
    "            Datemod.append(data.orderDate.iloc[i])\n",
    "        else:\n",
    "            d = data.orderDate.iloc[i]\n",
    "            d = datetime.datetime.strptime(d, \"%Y-%m-%d\")\n",
    "            d = d- datetime.timedelta(days=1)\n",
    "            d = d.strftime(\"%Y-%m-%d\")\n",
    "            Datemod.append(d)\n",
    "    return Datemod\n",
    "\n",
    "def week_n(data,fall_18=0,spring_19=0,spring_18=0):\n",
    "    '''\n",
    "    This function takes the whole dataset, fall/spring boolean and returns a new column of week_number\n",
    "    in a fall/spring semester stored in a list.\n",
    "    '''\n",
    "    week_num = []\n",
    "    if fall_18==1:\n",
    "        start = datetime.datetime.strptime('2018-09-02', \"%Y-%m-%d\")\n",
    "    if spring_19==1:\n",
    "        start = datetime.datetime.strptime('2019-01-13', \"%Y-%m-%d\")\n",
    "    if spring_18 == 1:\n",
    "        start = datetime.datetime.strptime('2018-01-14', \"%Y-%m-%d\")\n",
    "    for i in range(len(data)):\n",
    "        d = data['Datemod'].iloc[i]\n",
    "        d = datetime.datetime.strptime(d, \"%Y-%m-%d\")\n",
    "        week_num.append(int(np.ceil((d - start)/ datetime.timedelta(days=7))))\n",
    "    return week_num\n",
    "\n",
    "def week_n_au(data,fall_18=0,spring_19=0,spring_18=0):\n",
    "    '''\n",
    "    THis function takes a data frame about daily active user amounts, fall/spring boolean\n",
    "    and returns the corresponding week_number in the semester that will refer to \n",
    "    this value stored in a list.\n",
    "    '''\n",
    "    week_num = []\n",
    "    if fall_18==1:\n",
    "        start = datetime.datetime.strptime('2018-08-19', \"%Y-%m-%d\")\n",
    "    if spring_19==1:\n",
    "        start = datetime.datetime.strptime('2018-12-30', \"%Y-%m-%d\")\n",
    "    if spring_18==1:\n",
    "        start = datetime.datetime.strptime('2017-12-31', \"%Y-%m-%d\")\n",
    "    for i in range(len(data)):\n",
    "        d = data['formattedDate'].iloc[i]\n",
    "        d = datetime.datetime.strptime(d, \"%Y-%m-%d\")\n",
    "        week_num.append(int(np.ceil((d - start)/ datetime.timedelta(days=7))))\n",
    "    return week_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_item_19spring(filename,enddate,pred_enddate,dau_filename):\n",
    "    \"\"\"\n",
    "    This funciton takes (1) csv filename of a top10 item (2) the cutoff date of order records (3) the last date of our prediction \n",
    "    (4) csv filename of daily active users upto the cutoff date as arguments, and returns an organized dataframe \n",
    "    including all features for a top10 item.\n",
    "    \"\"\"\n",
    "    data= pd.read_csv(filename,dtype = str,index_col=0)\n",
    "    item_name = data.itemName.iloc[0]\n",
    "    data = data[['orderDate', 'orderTime','itemQuantity', 'itemPricePerUnit', 'itemPriceTotal','mealPlanBoolean','circuitName', 'dayOfWeek','discountPercent', \n",
    "            'discountPerUnit','discountTotal', 'discountType','deliveryDate']]\n",
    "    data.orderDate = data.orderDate.apply(unifDate)\n",
    "    data.mealPlanBoolean = data.mealPlanBoolean.apply(lambda x: 1 if x=='true' else 0)\n",
    "    data.itemQuantity = data.itemQuantity.apply(int)\n",
    "    data.itemPricePerUnit = data.itemPricePerUnit.apply(float)\n",
    "\n",
    "    data['Datemod'] = pd.Series(datemod(data),index=data.index)\n",
    "    data = data[[ 'itemQuantity', 'itemPricePerUnit',  'circuitName', 'dayOfWeek', 'Datemod']]\n",
    "    data.set_index('Datemod', drop = False,inplace=True)\n",
    "    data = data.loc[:enddate]\n",
    "    daily_sales = data.groupby(by = 'Datemod').itemQuantity.sum()\n",
    "\n",
    "    new_index = pd.date_range(start='2019-01-14', end=pred_enddate, freq='D')\n",
    "    new_index=new_index.astype(str)\n",
    "\n",
    "    data.drop_duplicates(subset=['Datemod'], keep='first', inplace = True)\n",
    "    data = data.reset_index(drop=True)\n",
    "    data.index=data.Datemod\n",
    "    data = data.reindex(new_index)\n",
    "    data.Datemod = data.index\n",
    "\n",
    "    #Monday: 0 Sunday:6\n",
    "    data['dayOfWeek'] = data.Datemod.apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%d\").weekday())\n",
    "    data = data[data['Datemod']>='2019-01-14']\n",
    "\n",
    "    # Add cycle_number feature\n",
    "    data['cycle_n'] = np.where(((data['dayOfWeek'] == 0)|(data['dayOfWeek'] == 1)|(data['dayOfWeek'] == 2)),1,\n",
    "             np.where(((data['dayOfWeek'] == 3)|(data['dayOfWeek'] == 4)),2,3))\n",
    "    # Add actual 19spring week number\n",
    "    data['week_n'] = pd.Series(week_n(data,spring_19=1),index=data.index)\n",
    "\n",
    "    df_sales=pd.DataFrame(daily_sales)\n",
    "    df_sales = df_sales.rename(columns={'itemQuantity':'sales'})\n",
    "\n",
    "    data = pd.merge(data,df_sales,how='left',left_index=True,right_index=True)\n",
    "    data1=data[data.columns.difference(['circuitName','itemQuantity'])]\n",
    "    \n",
    "    # Adjust daily sales according to discount situation\n",
    "    perc = pd.read_pickle(\"C:/Users/Baoyp/Documents/2019 Spring/GU project/premodel/premodelDF.pkl\")\n",
    "    discount = pd.read_pickle(\"C:/Users/Baoyp/Documents/2019 Spring/GU project/discount.p\")\n",
    "    discount['discount'] = 1\n",
    "    discount_item=discount[discount['item_name']==item_name][['Date','discount']]\n",
    "\n",
    "    data1 = data1.merge(discount_item,how='left',left_on='Datemod',right_on='Date')\n",
    "    del data1['Date']\n",
    "    data1.fillna(0,inplace=True)\n",
    "    perc_item=perc[perc['name']==item_name][['weekday','factor']]\n",
    "    data1 = data1.merge(perc_item,how='left',left_on='dayOfWeek',right_on='weekday')\n",
    "    del data1['weekday']\n",
    "    weekly_sales=data1.groupby('week_n').sales.sum()\n",
    "    data1.discount = data1.discount.apply(int)\n",
    "    idx = data1.index[data1['discount']]==1\n",
    "    if len(data1.loc[idx]) == 0:\n",
    "        pass\n",
    "    elif len(data1.loc[idx]) == 1:\n",
    "        data1.loc[idx,'sales']=data1.loc[idx]['factor']*weekly_sales.loc[data1.loc[idx]['week_n']]\n",
    "    elif len(data1.loc[idx]) >1:\n",
    "        data1.loc[idx,'sales']=data1.loc[idx]['factor'].values*weekly_sales.loc[data1.loc[idx]['week_n']].values\n",
    "\n",
    "    # Get sales data\n",
    "    sales = data1.groupby(by = ['week_n', 'cycle_n']).sales.sum()\n",
    "    sales = sales.reset_index(level=['week_n', 'cycle_n'])\n",
    "\n",
    "    df1 = pd.read_csv('Academic_Calander_Spring_2019.csv')\n",
    "    df1 = df1.iloc[:110]\n",
    "    df1.deliveryDate = df1.deliveryDate.apply(lambda x: datetime.datetime.strptime(x, \"%m/%d/%y\").strftime(\"%Y-%m-%d\"))\n",
    "    df1 = df1.rename(columns = {'deliveryDate':'Datemod','class':'classes'})\n",
    "    data1 = pd.merge(data1,df1, on = 'Datemod', how='left')\n",
    "\n",
    "    # Get exam days feature\n",
    "    df_exam = data1.groupby(['week_n','cycle_n']).exam.sum()\n",
    "    df_exam = pd.DataFrame(df_exam,columns=['exam'])\n",
    "    df_exam.exam = np.where(df_exam.exam>0,1,0)\n",
    "\n",
    "    # Get class days feature\n",
    "    df_class = data1.groupby(['week_n','cycle_n']).classes.sum()\n",
    "    df_class = pd.DataFrame(df_class,columns=['classes'])\n",
    "    # Get sports days feature\n",
    "    df_sport = data1.groupby(['week_n','cycle_n']).sports.sum()\n",
    "    df_sport = pd.DataFrame(df_sport,columns=['sports'])\n",
    "    df_sport.sports = np.where(df_sport.sports>0,1,0)\n",
    "\n",
    "    data1 = data1[['cycle_n', 'itemPricePerUnit','week_n']]\n",
    "    # Get price feature, it hasn't been used yet\n",
    "    price = data.groupby(by = ['week_n', 'cycle_n']).itemPricePerUnit.min()\n",
    "    price = price.reset_index(level=['week_n', 'cycle_n'])\n",
    "\n",
    "    final = pd.merge(data1, sales, how = 'left', on = ['week_n', 'cycle_n'])\n",
    "    final.drop_duplicates(subset=['week_n', 'cycle_n'], keep='first', inplace = True)\n",
    "    final = final.reset_index(drop=True)\n",
    "    final = final[['week_n','cycle_n','itemPricePerUnit','sales']]\n",
    "    final = final.rename(columns = {'itemPricePerUnit':'price'})\n",
    "    final.sales = np.where((final['cycle_n']==1),final['sales']/3,final['sales']/2)\n",
    "\n",
    "    final = final[~(final['week_n']==8)]\n",
    "    final = final[~(final['week_n']==9)]\n",
    "\n",
    "    # Get 2-week lag feature\n",
    "    final['last_2_week_sales'] = final['sales']\n",
    "    final['last_2_week_sales']=final['last_2_week_sales'].shift(6)\n",
    "\n",
    "    final = pd.merge(final,df_class,on = ['week_n','cycle_n'], how='left')\n",
    "    final = pd.merge(final,df_exam,on = ['week_n','cycle_n'], how='left')\n",
    "    final = pd.merge(final,df_sport,on = ['week_n','cycle_n'], how='left')\n",
    "\n",
    "    main_data = pd.get_dummies(final, prefix='Cycle_', columns=['cycle_n'])\n",
    "\n",
    "    # Get weekly active users feature\n",
    "    au = pd.read_csv(dau_filename)\n",
    "    au = au.iloc[9:]\n",
    "    au = au.reset_index(drop=True)\n",
    "    au = au[au['formattedDate']<=enddate]\n",
    "    au['week_n'] = pd.Series(week_n_au(au,spring_19=1),index=au.index)\n",
    "\n",
    "    if final.iloc[-1]['week_n']==10:\n",
    "        for i in range(42,49):\n",
    "            au.loc[i,'week_n']=10\n",
    "            \n",
    "    if final.iloc[-1]['week_n']==11:\n",
    "        for i in range(42,49):\n",
    "            au.loc[i,'week_n']=11\n",
    "\n",
    "    au = au.rename(columns={'f0_':'n_users'})\n",
    "    df_au = au.groupby('week_n').n_users.sum()\n",
    "    df_au = df_au.reset_index(drop=False)\n",
    "\n",
    "\n",
    "    main_data = pd.merge(main_data,df_au,how='left',on='week_n')\n",
    "\n",
    "    # Get average sales over last three cycles feature\n",
    "    df_weekly = main_data.groupby(by=['week_n']).sales.mean()\n",
    "\n",
    "    avg_over_3_cycles = df_weekly.shift(2)\n",
    "    avg_over_3_cycles=avg_over_3_cycles.reset_index()\n",
    "    avg_over_3_cycles=avg_over_3_cycles.rename(columns={'sales':'avg_over_last_3_cycles'})\n",
    "\n",
    "    main_data = pd.merge(main_data,avg_over_3_cycles[['week_n','avg_over_last_3_cycles']], on = 'week_n',how = 'left')\n",
    "    main_data = main_data.drop('price',axis=1)\n",
    "    main_data['item_name'] = item_name\n",
    "    main_data = main_data.dropna()\n",
    "\n",
    "    # Add sellout feature\n",
    "    sellout = pd.read_pickle('C:/Users/Baoyp/Documents/2019 Spring/GU project/week10(3.18-3.24)/sellout_df_spring19.pkl')\n",
    "    sellout = sellout[(sellout['week_n']!=0)&(sellout['productName']==item_name)]\n",
    "    sellout['ref_week_n'] = np.where(sellout['week_n']!=7,sellout['week_n']+2,sellout['week_n']+4)\n",
    "    del sellout['week_n']\n",
    "    sellout = sellout.rename(columns={'ref_week_n':'week_n'})\n",
    "    del sellout['sellout']\n",
    "    sellout = sellout.rename(columns={'productName':'item_name'})\n",
    "\n",
    "    main_data['cycle_n'] = np.where(main_data['Cycle__1']==1,1, np.where(main_data['Cycle__2']==1,2,3))\n",
    "    main_data = pd.merge(main_data,sellout,how='left')\n",
    "    main_data= main_data.rename(columns = {'75_percent':'sellout'})\n",
    "    main_data['sellout*2_week_lag']=main_data['last_2_week_sales']*main_data['sellout']\n",
    "    main_data = main_data[['week_n', 'sales', 'last_2_week_sales', 'classes', 'exam', 'sports',\n",
    "           'Cycle__1', 'Cycle__2', 'Cycle__3', 'n_users', 'avg_over_last_3_cycles','sellout','sellout*2_week_lag','cycle_n','item_name']]\n",
    "    \n",
    "    return main_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filename='C:/Users/Baoyp/Documents/2019 Spring/GU project/by2.17/top1.csv'\n",
    "# enddate = '2019-02-17'\n",
    "# dau_filename = 'DAU_for_week7.csv'\n",
    "\n",
    "enddate = '2019-03-03'\n",
    "pred_enddate = '2019-03-24'\n",
    "dau_filename = 'C:/Users/Baoyp/Documents/2019 Spring/GU project/week10(3.18-3.24)/DAU_for_week10.csv'\n",
    "filenames = [f'C:/Users/Baoyp/Documents/2019 Spring/GU project/week10(3.18-3.24)/top{i}.csv' for i in range(1,11)]\n",
    "dfs = [clean_item_19spring(filename,enddate,pred_enddate,dau_filename) for filename in filenames]\n",
    "main_data = pd.concat(dfs)\n",
    "main_data1 = main_data.sort_values(by=['week_n'])\n",
    "main_data1.reset_index(inplace=True,drop=True)\n",
    "main_data1.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit into the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CHANGED!\n",
    "final_model = pickle.load(open('final_log_model_stats.p',\"rb\" ))\n",
    "df = main_data1.copy() #pickle.load(open('final_dataset_19spring_week8.p',\"rb\" ))\n",
    "name_list = df.item_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_week_n=10\n",
    "test_data =df[df['week_n']==pred_week_n]\n",
    "test_data = test_data.sort_values(by=['item_name','cycle_n'],axis=0)\n",
    "del test_data['sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_data = test_data [(test_data['item_name']== 'Mac and Cheese')| (test_data['item_name']== 'Mexican Burrito Bowl')| \n",
    "            (test_data['item_name']== 'Penne alla Vodka')|(test_data['item_name']== 'Pad Thai with Chicken') |\n",
    "            (test_data['item_name']== 'Roasted Chicken Plate')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_data['sales']=np.log(test_data['sales'])\n",
    "test_data['n_users'] = np.log(test_data['n_users'])\n",
    "test_data['avg_over_last_3_cycles']=np.log(test_data['avg_over_last_3_cycles'])\n",
    "test_data['last_2_week_sales'] = np.log(test_data['last_2_week_sales'])\n",
    "test_data['sellout*2_week_lag'] = test_data['sellout']*test_data['last_2_week_sales']\n",
    "test_data['week_n'] = np.log(test_data['week_n'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  The order of variables here muse be aligned with the order of input variables in the final model\n",
    "### CHANGED\n",
    "X_test = test_data[['Cycle__2', 'Cycle__3', 'avg_over_last_3_cycles', 'exam', 'last_2_week_sales', 'n_users', 'sellout' , 'week_n']]\n",
    "X_test = sm.add_constant(X_test, has_constant='add')\n",
    "Y_pred =  final_model.predict(X_test)\n",
    "####### test_data['Y_pred'] = np.exp(Y_pred)\n",
    "test_data['Y_pred'] = Y_pred\n",
    "########\n",
    "#test_data['Final_pred'] = np.where((test_data.cycle_n==1),test_data['Y_pred']*3,test_data['Y_pred']*2)\n",
    "pred_output = test_data[['item_name','Y_pred','week_n','cycle_n']]\n",
    "pred_output = pred_output.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CHANGED\n",
    "def simu_new(row,alpha):\n",
    "    '''\n",
    "    This function takes output from main model as lambda, and return order quantity Q that keeps wastage level at 0.3.\n",
    "    '''\n",
    "    Ys = np.random.normal(row['mean'],row['std'],1000)\n",
    "    Xs = np.exp(Ys)\n",
    "    Xs = sorted(Xs)\n",
    "    return Xs[int(1000*alpha)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "###CHANGED\n",
    "df_loc = pred_output['Y_pred']\n",
    "summary = final_model.get_prediction(X_test).summary_frame(alpha=0.05)\n",
    "df_se=(summary['obs_ci_upper']-summary['obs_ci_lower'])/(2*1.96)\n",
    "df_se = df_se.reset_index(drop=True)\n",
    "#df_se = np.where(df_se.index%3==0,df_se*3,df_se*2)\n",
    "df_dist = pd.DataFrame({'mean':df_loc, 'std':df_se})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9066000000000001"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###CHANGED\n",
    "Q_GU = 990\n",
    "error=list()\n",
    "alpha_array = np.linspace(0,1,10000,endpoint=False)\n",
    "for alpha in alpha_array:\n",
    "    Qs = df_dist.apply(lambda x: simu_new(x,alpha),axis=1)\n",
    "    Qs = np.where(Qs.index%3==0,Qs*3,Qs*2)\n",
    "    Q = np.sum(Qs)\n",
    "    error.append(Q-Q_GU)\n",
    "alpha_opt = alpha_array[np.argmin(np.absolute(error))]\n",
    "alpha_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01104908])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(error)[(alpha_array==alpha_opt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Qs = df_dist.apply(lambda x: simu_new(x,alpha_opt),axis=1)\n",
    "Qs = np.where(Qs.index%3==0,Qs*3,Qs*2)\n",
    "pred_output['simu_result'] = Qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_output.to_csv('week10_mainmodel_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### UP TO HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
